{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer는 2017년에 도입되어서 많이 쓰이는 모델이다.\n",
        "Transformer에 해당하는 대표적인 모델 3개는 다음과 같다.\n",
        "\n",
        "1. GPT (auto regressive)\n",
        "2. BERT (auto encoding)\n",
        "3. BART/T5 (sequence to sequence)"
      ],
      "metadata": {
        "id": "XtSiNd4ngbYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer 기반의 모델들은 모두 self-supervised learning 방식으로 학습된다. 여기서 self-supervised learning이란 우리가 label을 붙일 필요없이 input으로 자동 학습되는 것을 의미한다.\n",
        "\n",
        "이런류의 모델은 학습된 언어에 대한 통계적 이해를 발전시키지만, 특정 practical한 task에 대해서는 유용하지 않다. 이러한 이유로, pre-trained model이 transfer learning이라는 과정을 거친다. 이 과정에서 모델은 supervised 방식 (인간이 라벨링한 데이터)으로 fine tuned 된다.\n",
        "\n",
        "예시 중 하나는 이전의 n개 단어를 읽어서 문장의 다음 단어를 예측하는 것이다. 이를 causal language modeling이라고 한다. 왜냐하면 output이 이전과 현재의 input에 의존적이지만 미래의 것에는 의존하지 않기 때문이다."
      ],
      "metadata": {
        "id": "anro9lagL5EM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 트랜스포머는 기본적으로 큰 모델\n",
        "DistilBERT와 같은 몇몇 모델을 제외하고, 일반적으로 좋은 성능을 내기 위해서는 데이터의 양 뿐만 아니라 model의 사이즈를 늘리는 것이다.\n"
      ],
      "metadata": {
        "id": "oRJCcRtpL4_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ㅆㄱ문래귿ㄱ ㅡㅐㅇ딘](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png)\n",
        "- https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt"
      ],
      "metadata": {
        "id": "pov5Bm1VL47L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "하지만, 일반적으로 큰 모델을 학습하기 위해서, 많은 양의 데이터가 필요하다. 이는 시간과 compute 자원 측면에서 매우 비싸다. 이는 이산화탄소 배출과 같은 환경 문제로도 직결될 수 있음."
      ],
      "metadata": {
        "id": "fC8iYTWlL45E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer learning\n",
        "\n",
        "Pretraining은 scratch부터 model을 학습시키는 것을 의미한다.\n",
        "- weight가 랜덤하게 initialized되어있음.\n",
        "- 어떠한 prior knowledge가 없이 training을 시작\n",
        "\n",
        "이러한 pretraining은 보통 매우 많은 data를 가지고 행해진다. 따라서 우리는 많은 양의 데이터가 필요하고, training은 몇주까지 걸릴 수 있다.\n",
        "\n",
        "fine-tuning은 pre-trained된 모델 이후에 진행되는 학습이다. fine-tuning을 하기 위해, 먼저 pre-trained model이 필요하다. 그리고 원하는 task에 대해 특정 dataset으로 추가적인 training을 진행한다.\n",
        "\n",
        "**잠깐, 왜 그냥 처음부터(pre training 시점부터) 모델을 이 특정 task에 학습시키지 않는건가?** 그 이유는 아래와 같다.\n",
        "- pre trained model은 이미 fine-tuning data와 어느정도 유사한 데이터를 통해 학습되었다. fine tuning 과정은 pre training과정에서 모델이 얻은 knowledge의 장점을 얻을 수 있다 (예를들어 NLP에서 pre trained model은 우리의 task에서의 언어에 대한 통계적 이해가 어느정도 있을 것이다. 화합물 생성도 마찬가지).\n",
        "- 이미 model이 매우 많은 데이터에 대해서 pretrained 되었기 때문에, fine tuning은 더 적은 데이터로 충분하다.\n",
        "- 학습을 위한 시간과 비용이 줄어든다.\n",
        "\n",
        "예를 들어, 영어에 대해서 학습된 pre-trained model을 arXiv corpus로 fine tuning해서 research 기반 모델을 만들 수 있다. fine-tuning은 적은 양의 데이터만을 요구하여, pre trained model이 얻은 knoweldege가 전이된다고 한다. 이로인해 transfer learning이라고 한다.\n",
        "\n",
        "Fine-tuning 모델은 경제적, 환경적, 시간적, 데이터 관점에서 매우 유용하다.\n",
        "\n",
        "그리고 데이터가 많지 않은 이상, 특정 task에 대해서 scratch부터 학습하는 것보다 trasnfer learning을 활용하는 것이 더 나은 결과를 줄 것이다.\n",
        "\n",
        "이것이 transfer learning (혹은 pre trained model)을 활용해야하는 이유이다.\n"
      ],
      "metadata": {
        "id": "clZDjrjiL41F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cewDBzHlL4y-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer의 기본적인 요소\n",
        "\n",
        "1. Encoder: input의 representation을 맡는다. input을 이해하는 task에 좋다. 예시로 sentence classification이나 named entity recognition이 있다.\n",
        "2. Decoder: sequence 생성을 담당한다. text generation task에 적절하다.\n",
        "3. encoder-decoder(혹은 sequence-to-sequence): input을 요구하는 generative task에 좋다. (예: translation, summarization)"
      ],
      "metadata": {
        "id": "pCv4VXR7L4wZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder와 Decoder 부분은 익숙하여 tutorial을 스킵하였다."
      ],
      "metadata": {
        "id": "Keo8X0UDnE1h"
      }
    }
  ]
}