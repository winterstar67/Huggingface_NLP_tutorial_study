{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNhVWLMd2RVt47rF+cbFNuq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c00d200ce44b41739d7baaf8e8ef5aae":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f0382b9cc8d44c02aaf7155b18c20c73","IPY_MODEL_5851f44a80454f5ca18602a2ecec7608","IPY_MODEL_f592588b6d5e4dcbb8c8d49b1670222d"],"layout":"IPY_MODEL_6baf96b6eb7446fc8d0b112497f2c3e0"}},"f0382b9cc8d44c02aaf7155b18c20c73":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_176df22fa40a4580914272dc497121fd","placeholder":"​","style":"IPY_MODEL_8f3cba376070484b8c2e23a0aff1ea61","value":"tokenizer_config.json: 100%"}},"5851f44a80454f5ca18602a2ecec7608":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52b6fd5deee14d909e454afe14c79fc3","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4a3eca1f6cfb43659fae95377c89786d","value":48}},"f592588b6d5e4dcbb8c8d49b1670222d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d6da5022e3e4e288aa9b5fd6604e813","placeholder":"​","style":"IPY_MODEL_17254ee9ca624a21a0efccb9dcaef453","value":" 48.0/48.0 [00:00&lt;00:00, 2.35kB/s]"}},"6baf96b6eb7446fc8d0b112497f2c3e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"176df22fa40a4580914272dc497121fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f3cba376070484b8c2e23a0aff1ea61":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52b6fd5deee14d909e454afe14c79fc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a3eca1f6cfb43659fae95377c89786d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1d6da5022e3e4e288aa9b5fd6604e813":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17254ee9ca624a21a0efccb9dcaef453":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c4c211616a44f0d9af1a84661c7c31d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bc48877c618441209bdfa9ea8997ec80","IPY_MODEL_cb6e72de6a9e46a4b5da0684426c62d8","IPY_MODEL_f6259b25526e4f1992df6d9396916ddd"],"layout":"IPY_MODEL_f8fff45d4ce04b10adc6a9ac78933955"}},"bc48877c618441209bdfa9ea8997ec80":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e824f1b0ee140ae8af8b38adabec98c","placeholder":"​","style":"IPY_MODEL_8f061d13e2a04443994e3bb537d0977f","value":"config.json: 100%"}},"cb6e72de6a9e46a4b5da0684426c62d8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e3ee7e0f61943a0b4df905a0fbd4701","max":629,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2edd48833cb64cac9063045ce37f9233","value":629}},"f6259b25526e4f1992df6d9396916ddd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a065832d543b44bfbf695b843a2d9b90","placeholder":"​","style":"IPY_MODEL_d1f64f9e87b146b5acc955e2444c757d","value":" 629/629 [00:00&lt;00:00, 38.3kB/s]"}},"f8fff45d4ce04b10adc6a9ac78933955":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e824f1b0ee140ae8af8b38adabec98c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f061d13e2a04443994e3bb537d0977f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e3ee7e0f61943a0b4df905a0fbd4701":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2edd48833cb64cac9063045ce37f9233":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a065832d543b44bfbf695b843a2d9b90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1f64f9e87b146b5acc955e2444c757d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"762ee6c744a94602ae3b932b0a18d7eb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_26f4e1826da447459f5081a09b3440e8","IPY_MODEL_c0629ed296204cb3ba2730ddef36f59c","IPY_MODEL_159b036267a440398eeb6f8eb9b9bde2"],"layout":"IPY_MODEL_012fb9ce923548f4b05bab4852fbba2f"}},"26f4e1826da447459f5081a09b3440e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a9fc4ca0a074b76b8bf4c101e85f788","placeholder":"​","style":"IPY_MODEL_ce30b9ed90bb4dae82c30b9166d4919e","value":"vocab.txt: 100%"}},"c0629ed296204cb3ba2730ddef36f59c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9117a6e65c3244a7b1d848240c141a8e","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d891bed011a74c08ac44b75c982fa571","value":231508}},"159b036267a440398eeb6f8eb9b9bde2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4287580b1e84415b6b13851d8bf3db1","placeholder":"​","style":"IPY_MODEL_91764b691deb48ff8daa524f87966d1e","value":" 232k/232k [00:00&lt;00:00, 2.12MB/s]"}},"012fb9ce923548f4b05bab4852fbba2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a9fc4ca0a074b76b8bf4c101e85f788":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce30b9ed90bb4dae82c30b9166d4919e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9117a6e65c3244a7b1d848240c141a8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d891bed011a74c08ac44b75c982fa571":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b4287580b1e84415b6b13851d8bf3db1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91764b691deb48ff8daa524f87966d1e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10b7192488fb4af8886dda9dba30c6b5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_25e512d3ca8f4b13b02e846a8524d1cd","IPY_MODEL_ac084dfdac034668b1a5b58a4cb6faa4","IPY_MODEL_fa9e1901d7ae456a9c668833574e18e2"],"layout":"IPY_MODEL_38dcc247e3cc4e51b7dc73921f038efd"}},"25e512d3ca8f4b13b02e846a8524d1cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e221c0cf474485e9cfe8c7e54f1c6d8","placeholder":"​","style":"IPY_MODEL_7b7eebe382744fd994adfabeb74c17ee","value":"model.safetensors: 100%"}},"ac084dfdac034668b1a5b58a4cb6faa4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac468ac344694033832fad013f7d8511","max":267832558,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5fa0c25ef56147bbbd9a5f93e323cb2d","value":267832558}},"fa9e1901d7ae456a9c668833574e18e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d5a303764e84be0b31b52feb1a5bf0d","placeholder":"​","style":"IPY_MODEL_30fef660d70242c096eb1a2c2d4e06b7","value":" 268M/268M [00:01&lt;00:00, 203MB/s]"}},"38dcc247e3cc4e51b7dc73921f038efd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e221c0cf474485e9cfe8c7e54f1c6d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b7eebe382744fd994adfabeb74c17ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac468ac344694033832fad013f7d8511":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fa0c25ef56147bbbd9a5f93e323cb2d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5d5a303764e84be0b31b52feb1a5bf0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30fef660d70242c096eb1a2c2d4e06b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Handling multiple sequences\n","- https://huggingface.co/learn/nlp-course/chapter2/5?fw=pt\n","\n","이전까지, 가장 간단한 사용법을 보았다. 하나의 짧은 sequence에 대해서 inference를 하였는데, 다음과 같은 의문점들이 있다.\n","- 어떻게 multiple sequence를 다루는가?\n","- 어떻게 다른 길이의 multiple sequence를 다루는가?\n","- vocabulary indices는 오직 model이 잘 다루도록 하는 input일 뿐인가? (이 부분은 잘 이해를 못했다.)\n","- 너무 긴 seqeunce라는 것도 있는가?\n","\n","어떠한 종류의 문제들이 있고 어떻게 transformer API를 사용해서 해결할지 보자."],"metadata":{"id":"hPZA5BU6royU"}},{"cell_type":"markdown","source":["## 1. Models expect a batch of inputs\n","\n","이전 section에서, 우리는 어떻게 sequences가 숫자의 list로 변환되는지 보았다. 이번에는 숫자 list를 tensor로 변환해서 model로 보내보자."],"metadata":{"id":"p_y_T6b3_40h"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","tokens = tokenizer.tokenize(sequence)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","input_ids = torch.tensor(ids)\n","\n","model(input_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":638,"referenced_widgets":["c00d200ce44b41739d7baaf8e8ef5aae","f0382b9cc8d44c02aaf7155b18c20c73","5851f44a80454f5ca18602a2ecec7608","f592588b6d5e4dcbb8c8d49b1670222d","6baf96b6eb7446fc8d0b112497f2c3e0","176df22fa40a4580914272dc497121fd","8f3cba376070484b8c2e23a0aff1ea61","52b6fd5deee14d909e454afe14c79fc3","4a3eca1f6cfb43659fae95377c89786d","1d6da5022e3e4e288aa9b5fd6604e813","17254ee9ca624a21a0efccb9dcaef453","2c4c211616a44f0d9af1a84661c7c31d","bc48877c618441209bdfa9ea8997ec80","cb6e72de6a9e46a4b5da0684426c62d8","f6259b25526e4f1992df6d9396916ddd","f8fff45d4ce04b10adc6a9ac78933955","6e824f1b0ee140ae8af8b38adabec98c","8f061d13e2a04443994e3bb537d0977f","4e3ee7e0f61943a0b4df905a0fbd4701","2edd48833cb64cac9063045ce37f9233","a065832d543b44bfbf695b843a2d9b90","d1f64f9e87b146b5acc955e2444c757d","762ee6c744a94602ae3b932b0a18d7eb","26f4e1826da447459f5081a09b3440e8","c0629ed296204cb3ba2730ddef36f59c","159b036267a440398eeb6f8eb9b9bde2","012fb9ce923548f4b05bab4852fbba2f","4a9fc4ca0a074b76b8bf4c101e85f788","ce30b9ed90bb4dae82c30b9166d4919e","9117a6e65c3244a7b1d848240c141a8e","d891bed011a74c08ac44b75c982fa571","b4287580b1e84415b6b13851d8bf3db1","91764b691deb48ff8daa524f87966d1e","10b7192488fb4af8886dda9dba30c6b5","25e512d3ca8f4b13b02e846a8524d1cd","ac084dfdac034668b1a5b58a4cb6faa4","fa9e1901d7ae456a9c668833574e18e2","38dcc247e3cc4e51b7dc73921f038efd","7e221c0cf474485e9cfe8c7e54f1c6d8","7b7eebe382744fd994adfabeb74c17ee","ac468ac344694033832fad013f7d8511","5fa0c25ef56147bbbd9a5f93e323cb2d","5d5a303764e84be0b31b52feb1a5bf0d","30fef660d70242c096eb1a2c2d4e06b7"]},"id":"4IqJmoZwBT_p","executionInfo":{"status":"error","timestamp":1705386734412,"user_tz":-540,"elapsed":13399,"user":{"displayName":"Chihyeon Jin","userId":"01748061193938596019"}},"outputId":"e5c772e0-f2a0-41b5-84de-730b529ea792"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c00d200ce44b41739d7baaf8e8ef5aae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c4c211616a44f0d9af1a84661c7c31d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"762ee6c744a94602ae3b932b0a18d7eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10b7192488fb4af8886dda9dba30c6b5"}},"metadata":{}},{"output_type":"error","ename":"IndexError","evalue":"too many indices for tensor of dimension 1","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b64448a994b5>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m         distilbert_output = self.distilbert(\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You cannot specify both input_ids and inputs_embeds at the same time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn_if_padding_and_no_attention_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mwarn_if_padding_and_no_attention_mask\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m   4114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4115\u001b[0m         \u001b[0;31m# Check only the first and last input IDs to reduce overhead.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4116\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4117\u001b[0m             warn_string = (\n\u001b[1;32m   4118\u001b[0m                 \u001b[0;34m\"We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"]}]},{"cell_type":"markdown","source":["- 에러가 나왔다. 무슨일일까?\n","\n","- 문제는 single sequence를 모델에 보냈다는 것이다.  \n"," Transformer 라이브러리의 모델들은 default로 multiple sentences를 보내는 것을 expect한다.  \n"," 여기서는 tokenizer가 sequence에 적용되어 뒤에서 수행하는 작업을 직접 해보았는데, 자세히 보면 tensor로 변환한 것 뿐만 아니라 그 위에 dimension을 추가했음을 알 수 있다.\n","\n"],"metadata":{"id":"UDEh0hg__4yx"}},{"cell_type":"code","source":["tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n","print(tokenized_inputs[\"input_ids\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XOaeo7D8CfEU","executionInfo":{"status":"ok","timestamp":1705386735550,"user_tz":-540,"elapsed":2,"user":{"displayName":"Chihyeon Jin","userId":"01748061193938596019"}},"outputId":"29758221-d056-4ea8-956f-099f8527320e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n","          2607,  2026,  2878,  2166,  1012,   102]])\n"]}]},{"cell_type":"markdown","source":["이번엔 새로운 dimension을 추가해서 다시 시도해보자."],"metadata":{"id":"4rdpOd6iCiLL"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","tokens = tokenizer.tokenize(sequence)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","input_ids = torch.tensor([ids])\n","print(\"Input IDs:\", input_ids.shape, input_ids)\n","\n","output = model(input_ids)\n","print(\"Logits:\", output.logits)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xe3HaYUjClxX","executionInfo":{"status":"ok","timestamp":1705386738239,"user_tz":-540,"elapsed":1264,"user":{"displayName":"Chihyeon Jin","userId":"01748061193938596019"}},"outputId":"dc4a3c8c-ae70-425a-84da-cf64a009fbe6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input IDs: torch.Size([1, 14]) tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n","          2026,  2878,  2166,  1012]])\n","Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["- Batching은 multiple sequence를 model로 한 번에 보내는 것이다.  \n","만약 1개의 sequence밖에 없다면, batch를 single sequence로 만들면 된다."],"metadata":{"id":"1nGO64Z8_4vm"}},{"cell_type":"code","source":["# Try it out!\n","batched_ids = [ids, ids]\n","batched_inputs = torch.tensor(batched_ids)\n","print(batched_inputs)\n","\n","model(batched_inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sINECIzcC9i9","executionInfo":{"status":"ok","timestamp":1705386739611,"user_tz":-540,"elapsed":3,"user":{"displayName":"Chihyeon Jin","userId":"01748061193938596019"}},"outputId":"77582655-184e-49a9-e63b-fae97e4a7d37"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n","          2026,  2878,  2166,  1012],\n","        [ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n","          2026,  2878,  2166,  1012]])\n"]},{"output_type":"execute_result","data":{"text/plain":["SequenceClassifierOutput(loss=None, logits=tensor([[-2.7276,  2.8789],\n","        [-2.7276,  2.8789]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["Batching은 모델이 multiple sequence를 다룰 수 있게 해준다.\n","Multiple sequence를 사용하는 것은 batch를 만드는 것 만큼이나 간단하다.\n","\n","\n","**하지만 또다른 issue가 있다.**   \n","\n","바로 여러 개의 sequence마다 길이가 다른 경우이다.  \n","만약 tensor로 작업을 하면, 우리는 반드시 rectangular shape로 만들어야 한다. 이를 해결하기 위해 `pad`라는 것을 추가할 것이다."],"metadata":{"id":"3rSpeWej_4tm"}},{"cell_type":"code","source":[],"metadata":{"id":"5TLG41nNDJfG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Padding the inputs\n","\n","다음과 같은 list of lists는 tensor로 변환될 수 없다. 왜냐하면 둘의 길이가 다르기 때문이다."],"metadata":{"id":"pK9Q7PaW_4rY"}},{"cell_type":"code","source":["batched_ids = [\n","    [200, 200, 200],\n","    [200, 200]\n","]\n","\n","torch.tensor(batched_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":223},"id":"-rP-d3YTE6yE","executionInfo":{"status":"error","timestamp":1705386741280,"user_tz":-540,"elapsed":367,"user":{"displayName":"Chihyeon Jin","userId":"01748061193938596019"}},"outputId":"8b59c779-22c5-4cd8-c4d1-e20360047a94"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"expected sequence of length 3 at dim 1 (got 2)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-43c838793d86>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m ]\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mValueError\u001b[0m: expected sequence of length 3 at dim 1 (got 2)"]}]},{"cell_type":"markdown","source":["이 문제를 다루기 위해, 우리는 `padding`을 사용해서 tensor를 rectangular하게 만들 것이다.  \n","padding은 sentence가 같은 길이를 가지도록 한다.  \n","이 때, padding token은 길이가 작은 sequence에 추가해서 문장의 길이가 같아지도록 한다.  \n","만약 10개의 문장은 단어 10개를, 문장 1개는 20개의 단어를 가진다고 하면, padding은 모든 문장을 20개의 단어를 가지게 할 것이다.\n","\n","예를 들어 위의 예시에 padding을 추가한다면"],"metadata":{"id":"Tpn_Gihc_4pZ"}},{"cell_type":"code","source":["padding_id = 100\n","\n","batched_ids = [\n","    [200, 200, 200],\n","    [200, 200, padding_id],\n","]\n","\n","torch.tensor(batched_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nFBliTAhFnUI","executionInfo":{"status":"ok","timestamp":1705386742438,"user_tz":-540,"elapsed":2,"user":{"displayName":"Chihyeon Jin","userId":"01748061193938596019"}},"outputId":"1bcaf3bf-cb97-4104-9705-9b38fff8bd3a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[200, 200, 200],\n","        [200, 200, 100]])"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["- 와 같이 된다."],"metadata":{"id":"DoSdenk7_4lS"}},{"cell_type":"markdown","source":["Huggingface에서는 `tokenizer.pad_toekn_id`를 통해서 padding token ID를 찾을 수 있다. 이를 두 문장에 대해서 적용해보고 model에 전달해보자."],"metadata":{"id":"DCo7OrdA_4jL"}},{"cell_type":"code","source":["model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequence1_ids = [[200, 200, 200]]\n","sequence2_ids = [[200, 200]]\n","batched_ids = [\n","    [200, 200, 200],\n","    [200, 200, tokenizer.pad_token_id],\n","]\n","\n","print(model(torch.tensor(sequence1_ids)).logits)\n","print(model(torch.tensor(sequence2_ids)).logits)\n","print(model(torch.tensor(batched_ids)).logits)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lf3ZqJWqGmtT","executionInfo":{"status":"ok","timestamp":1705386745371,"user_tz":-540,"elapsed":1385,"user":{"displayName":"Chihyeon Jin","userId":"01748061193938596019"}},"outputId":"7310ffab-0f31-417c-f9a0-aa21d92951ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n","tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n","tensor([[ 1.5694, -1.3895],\n","        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["위의 결과에서 이상한 점이 하나 있다. sequence1_ids와 sequence2_ids는 단일 sequence로 넣은 것이고, batched_ids는 sequence1_ids와 sequence2_ids를 동시에 넣은 것 뿐이지, 둘은 같은 결과를 내야한다.\n","\n","batched_ids의 첫번째 줄의 결과는 sequence1_ids와 같아야하고 batched_ids의 두번째 줄의 결과는 sequence2_ids와 같아야 한다.\n","\n","그런데, batched_ids의 두번째 줄과 sequence2_ids는 결과가 다르다. 그 이유는 transformer 모델에 있는 attention layer 때문이다. 이는 padding token까지 고려하기 때문인데, sequence2_ids와 batched_ids가 같은 결과를 내려면, attention layer가 padding token을 무시하도록 만들 필요가 있다. 이는 `attention mask` 라는 것을 적용해서 해결할 수 있다.\n"],"metadata":{"id":"Ci4oUD03HPnv"}},{"cell_type":"code","source":[],"metadata":{"id":"mhciEzZEGb0X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Attention masks\n","\n","`Attention masks`는 input IDs tensor와 동일한 shape를 가진 tensor이고, 안의 값은 오직 0아니면 1로 이루어져있다. 1은 해당 위치에 있는 token이 attention layer에서 attend되어야 하는 부분이라는 것을 가르키고, 0은 attend해선 안되는 부분이라는 것을 가르킨다.\n","\n","이전 예시에 attention mask를 적용해서 완성해본다."],"metadata":{"id":"sBzxI9hk_4hM"}},{"cell_type":"code","source":["model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequence1_ids = [[200, 200, 200]]\n","sequence2_ids = [[200, 200]]\n","batched_ids = [\n","    [200, 200, 200],\n","    [200, 200, tokenizer.pad_token_id],\n","]\n","\n","attention_mask = [\n","    [1, 1, 1],\n","    [1, 1, 0],\n","]\n","\n","\n","outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n","\n","print(model(torch.tensor(sequence1_ids)).logits)\n","print(model(torch.tensor(sequence2_ids)).logits)\n","print(outputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"keeDjwoT95KS","executionInfo":{"status":"ok","timestamp":1705386756333,"user_tz":-540,"elapsed":1728,"user":{"displayName":"Chihyeon Jin","userId":"01748061193938596019"}},"outputId":"71ed3728-f7ce-4b75-b1b7-f3e068c5da81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n","tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n","SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5694, -1.3895],\n","        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"]}]},{"cell_type":"markdown","source":["- 이제 동일한 결과가 나왔다.\n","\n"],"metadata":{"id":"awmqzGUu_4fE"}},{"cell_type":"code","source":[],"metadata":{"id":"jd4TfVkV-Q6P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. 더 긴 sequence에 대해서"],"metadata":{"id":"V9usVyKj_4dG"}},{"cell_type":"markdown","source":["Transformer 모델의 경우, sequence length의 길이에 대해 한계점이 있다. 대부분의 모델은 최대 512 혹은 1024개의 토큰까지 다룰 수 있고, 만약 더 긴 sequence를 요구하면 crash가 일어날 수 있다. 이에 대해 2가지 solution이 있다.\n","\n","1. 더 긴 sequence 길이를 지원하는 모델을 사용한다.\n","2. 사용자의 sequence를 중간까지만 받고 자른다.\n","\n","모델마다 서로 다른 sequence 길이를 받을 수 있고 몇몇 긴 sequence에 특화된 모델도 있다. Longformer와 LED가 긴 문장을 다룰 수 있는 모델의 예시이다. 만약 매우 긴 sequence를 요구하는 task를 다룬다면, 이러한 모델들을 보는 것도 좋다.\n","\n","그렇지 않으면 문장을 중간에 자르는 것인데, 이러한 경우에는 `max_sequence_length`를 사용할 수 있다."],"metadata":{"id":"DzqPOWJi_4a-"}},{"cell_type":"code","source":["sequence = sequence[:max_sequence_length]"],"metadata":{"id":"IdPBu3Db_0GL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 튜토리얼 영상"],"metadata":{"id":"Hn6AXw-BC-Hk"}},{"cell_type":"markdown","source":["1. Truncate는 중요한 정보의 손실을 가져올 수 있다.  \n","2. padding 토큰은 아무거나 쓰는 것이 아닌, pre training을 할 때 사용한 것을 사용해야한다.\n","3. 우리가 굳이 padding token 넣지 않아도 `AutoTokenizer`를 사용할 때 `tokenizer(sentences, padding=True)`를 하면 padding을 포함한 tokenizing이 된다."],"metadata":{"id":"RYKI8vnrDACu"}}]}